
\section{Results}

The implementation of software engineering principles of as separation of concerns (SoC) and modularity in deep learning projects results in a well-structured illustrated below. 
\dirtree{%
.1 /.
.2 models.
.3 \texttt{\_\_init\_\_.py}.
.2 experiments.
.3 \texttt{\_\_init\_\_.py}.
.2 trainers.
.3 \texttt{\_\_init\_\_.py}.
.2 data.
.3 \texttt{\_\_init\_\_.py}.
}
\vspace{0.7em}


The modular architecture of a deep learning project, characterized by the structured folder system, separates the key components into four distinct modules: models, experiments, trainers, and data. This enhances code legibility and comprehensibility, enabling other researchers to quickly access the desired code components. This approach leads to the formation of three standalone modules for models, trainers, and data, each possessing the ability to operate independently. Meanwhile, the fourth module, experiments, functions to harmonize the three modules and provide a unified and comprehensive experiment. The modular structure of independence further enables the sharing of models, trainers, and data loaders across various projects and experiments, thereby enhancing the code's reusability and scalability. 

\subsection{Case Study}
 To demonstrate the effectiveness of the convention, we present a case study of a deep learning project built using Yerbamate, and compare it to an existing project that did not adhere to the convention. Our results show that the project built with Yerbamate had better code quality, was more modular, and was easier to maintain and reproduce.
 
\subsubsection{Example Project Structure}

The following illustration presents a sample folder structure for a deep learning vision project\footnote{ \url{https://github.com/oalee/deep-vision}}. It's important to note that each folder represents a modular code component with a specific concern, which is demonstrated by the presence of \verb|__init__.py| within the folder.

\dirtree{%
.1 /.
.2 data.
.3 cifar10.
.4 \texttt{data\_loader.py}.
.4 \texttt{\_\_init\_\_.py}.
.3 cifar100.
.4 \texttt{data\_loader.py}.
.4 \texttt{\_\_init\_\_.py}.
.3 \texttt{\_\_init\_\_.py}.
.2 models.
.3 resnet.
.4 \texttt{fine\_tune.py}.
.4 \texttt{resnet.py}.
.4 \texttt{\_\_init\_\_.py}.
.3 efficientnet.
.4 \texttt{efficientnet.py}.
.4 \texttt{\_\_init\_\_.py}.
.3 \texttt{\_\_init\_\_.py}.
.2 trainers.
.3 classification.
.4 \texttt{pl\_classification\_module.py}.
.4 \texttt{\_\_init\_\_.py}.
.3 \texttt{\_\_init\_\_.py}.
.2 experiments.
.3 \texttt{resnet\_cifar10.py}.
.3 \texttt{resnet\_cifar100.py}.
.3 \texttt{fine\_tune\_resnet\_cifar.py}.
.3 \texttt{keras\_efficient\_net\_cifar.py}.
.3 \texttt{\_\_init\_\_.py}.
.2 \texttt{\_\_init\_\_.py}.
}
\vspace{2em}
Each top-level module can be divided into multiple sub-modules, allowing for the separation of individual components within each module. For instance, the models module can comprise sub-modules such as ResNet (\cite{resnet}), ViT (\cite{dosovitskiy2020vit}), CvT (\cite{wu2021cvt}), and others, implemented as independent components. Likewise, the data module can encompass sub-modules such as data loaders, preprocessing pipelines, and post-processing routines. This modular design promotes increased organization and manageability of the codebase, contributing to the efficiency and effectiveness of the deep learning project.


\subsection{Yerbamaté: An Open Science Python Framework}
Yerbamaté\footnote{\url{https://github.com/oalee/yerbamate}} is a result of the study on reproducibility and the importance of modularity in AI and machine learning projects. Yerbamaté is an open source open science framework designed to streamline and simplify the development and management of artificial intelligence and machine learning projects. Built around the principles of modularity and separation of concerns, Yerbamaté provides a convenient and efficient means of adding source code and dependencies to projects.

Modularity allows for a clean and organized codebase, making it easier to maintain, scale, and reuse the code in the future. In addition, Yerbamaté provides an easy-to-use interface for sharing the source code of models, trainers, data loaders, and experiments between modular projects, enabling greater flexibility and collaboration. Furthermore, adopting the Yerbamaté framework ensures that all projects adhering to its principles are readily compatible with Colab\footnote{\url{https://colab.research.google.com/github/oalee/yerbamate/blob/main/deep_learning.ipynb}}, a cloud-based platform widely used for machine learning experimentation and development.

Yerbamaté also provides support for full customizability and reproducibility of results through the inclusion of dependencies in your project. The tool supports pip and conda for dependency management, making it easy to manage and install the necessary dependencies for your project.
% Additionally, Yerbamaté is fully compatible with python, and can be used with popular libraries such as PyTorch/Lightning, TensorFlow/Keras, JAX/Flax, Huggingface/transformers. Another feature of Yerbamaté is its convenient environment management through the Yerbamaté Environment API. This API allows for the creation and management of virtual environments, ensuring that each experiment is run in an isolated environment with the necessary dependencies. 
For a comprehensive understanding of the Yerbamaté, see the documentation\footnote{\url{https://oalee.github.io/yerbamate/}}.


\subsubsection{Flexibility and Customization}
One of the primary challenges in the development of Yerbamaté was ensuring flexibility and customization.
To offer increased flexibility, Yerbamaté does not impose any restrictions on additional module names, allowing researchers to utilize their preferred module names for custom tasks. For instance, in the case of analyzing neural network weights and activations, researchers can use the "analyze" module and share code under that name.  Additionally, Yerbamaté is designed to be framework-agnostic, meaning it can work out of the box with any machine learning framework or library.  Moreover, Yerbamaté offers compatibility with Python and Python can be used for running experiments. 

\subsubsection{Yerbamaté Command Line}
The Yerbamaté command line provides useful utility functions and support for modular projects, which can facilitate the development of machine learning models.
Here is a list of the key Yerbamaté command line options:

\begin{itemize}
\item \textbf{mate init module\_name}: Initializes a new empty modular project skeleton with the given module name. 

\item \textbf{mate install url -y\textbar n\textbar o pm}: Installs a module from a git repository. Supports multiple formats for the repository URL. The flags -y, -n, and -o specify whether to skip confirmation, skip installing python dependencies, and overwrite existing code modules, respectively. The pm argument specifies the package manager to use.
\item \textbf{mate list}: Lists all available modules in the project. 
\item \textbf{mate exports}: Generates dependencies for reproduciblity and sharing.
\item \textbf{mate test exp\_module exp}: Runs the experiment specified by exp in the module exp\_module. Equivalent to python -m root\_module.exp\_module.exp test.
\item \textbf{mate train exp\_module exp}: Runs the experiment specified by exp in the module exp\_module. Equivalent to python -m root\_module.exp\_module.exp train.
\end{itemize}



Yerbamaté's install command is a crucial component of the tool, allowing for effortless installation of modularized projects that adhere to the principles of separation of concerns. With just one command, you can install a model along with its Python dependencies, making it easier to share and export modules. Additionally, Yerbamaté also supports the installation of coupled modules as whole modules. For example, you can install the source code of over 100 torch image models \footnote{\url{https://github.com/rwightman/pytorch-image-models/tree/main/timm/}} and over 30 implementation PyTorch vision in transformers \footnote{\url{https://github.com/lucidrains/vit-pytorch/tree/main/vit\_pytorch/}} directly into your project. However, the limitation of installing non-Yerbamaté projects is that Yerbamaté cannot install Python dependencies out of the box and this modules can only be installed as a whole, and sub-modules, such as models, are not installable as a standalone module since they are not independent.





\subsubsection{Yerbamaté Environment API}

The Yerbamaté Environment API is a tool designed to manage environment variables within a experiment. It prioritizes the use of an env.json file for storing environment variables, but if it is not found, it falls back to the operating system's environment variables. The API offers a convenient way to set, retrieve, and manage these variables in a centralized and organized manner. This can be particularly useful in storing and accessing environment-specific information, such as API keys, database URLs, and other sensitive data, thus ensuring that the application operates optimally regardless of the environment in which it is executed. The Yerbamaté Environment API can be easily accessed within experiments, providing a seamless and efficient method for managing environment variables within a project. 





\subsection{Experiment Configuration}

In the field of artificial intelligence and machine learning, defining hyperparameters and experiments plays a crucial role in the development and optimization of models \cite{wu2019hyperparameter}. The choice of format for defining experiments and hyperparameters can greatly impact the efficiency and flexibility of the experimentation process. After evaluating various formats such as JSON and TOML, it has been determined that Python is the most suitable format for defining experiments in AI and ML. Python provides a high level of expressiveness and versatility, allowing for easy modifications and adaptations of the experiment definition. In addition, Python offers a straightforward syntax for defining hyperparameters and allows for the integration of existing code and libraries. This greatly reduces the overhead associated with switching between different languages or systems, leading to a more streamlined and efficient experimentation process. Furthermore, the ability to use Python's powerful libraries and modules enhances the experimentation process by providing access to a vast range of tools and resources. This further enables the exploration of a wider range of hyperparameters and models, leading to a more comprehensive understanding of the problem at hand.

\subsubsection{No-Loop Python Experiment Configuration Convention}

The experiment configuration in this study adopts a restricted version of the Python language that only disallows the use of loops as a convention. The experiment configuration design choice is intended to maintain a hyperparameter-focused experiment format while promoting separation of concerns. The resulting approach provides several benefits, including increased flexibility and customization, as the configuration format can be adapted to various AI tasks, custom use cases, frameworks, and libraries. Additionally, by incorporating well-documented Python code, the readability of the configuration file is improved, making it more accessible to researchers and practitioners alike. Furthermore, this format is executable directly with Python, which makes it Turing complete. This property is particularly useful because it means that the format can include arbitrary computation and is capable of expressing any algorithm, enhancing its flexibility and power.



\subsection{Seperation of Concerns for AI}

In the field of software engineering, including artificial intelligence and machine learning, decoupling concerns, also known as separation of concerns, is a crucial design principle that helps to improve the maintainability, scalability, and reusability of code (\cite{mo2016decoupling,qian2006decoupling, pressman2010software}. The goal of decoupling concerns is to break down complex systems into smaller, modular components that can be independently developed, tested, and maintained (\cite{pressman2010software, mo2016decoupling, qian2006decoupling}). For example, in a typical deep learning experiment, the trainer component is responsible for training the model. The trainer component can be designed to receive either a string representing the dataset name or the actual dataset objects. The latter approach provides greater flexibility and customization, as the trainer component can operate on any dataset objects, regardless of the underlying dataset.


Another example of separation of concerns in AI is the data loading, training and augmentation process. In this context, hardcoding the dataset and augmentation within the training code leads to limitations in terms of flexibility and experimentation. A more effective approach is to pass the data objects directly to the trainer, rather than relying on a string representation of the dataset name. This approach offers greater flexibility in the data loading process, enabling more customization and control, as well as ease of experimentation with different datasets.

The separation of concerns also applies to the design of the model components. For example, a deep learning model can be implemented as a monolithic block of code or as a series of modular components, such as the encoder, decoder, and attention mechanism. The latter approach allows for greater flexibility and customization of the model, as each component can be modified or replaced without affecting the other components. Decoupling concerns is a crucial design principle in the field of artificial intelligence and machine learning. 


\subsubsection{Example Custom Data Preprocessing}
The modular structure of the Yerbamaté toolkit, coupled with its compatibility with pure Python, allows for the integration of custom data preprocessing pipelines with ease. By utilizing the Yerbamaté environment API, developers and researchers can readily access the data paths and results path for the destination of their processed data. For instance, the following project structure can utilize the command \texttt{python -m deepnet.data.my\_data.preprocessing} to execute a custom preprocessing pipeline. The flexibility offered by the python modularity enables users to efficiently tailor their preprocessing procedures to the specific requirements of their research or application, and the Yerbamaté toolkit can be used to share these pipelines effortlessly.

\dirtree{%
.1 deepnet.
.2 data.
.3 \texttt{\_\_init\_\_.py}.
.3 my\_data.
.4 \texttt{\_\_init\_\_.py}.
.4 preprocessing.
.5 \texttt{\_\_init\_\_.py}.
.5 preprocess.py.
.4 data\_loader.
.2 models.
.2 trainers.
.2 experiments.
}
