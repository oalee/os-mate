
\section{Results}

\subsection{Deep Learning Project Structure}

The implementation of software engineering principles of as separation of concerns (SoC) and modularity in deep learning projects results in a well-structured illustrated below. 
\dirtree{%
.1 /.
.2 models.
.3 \texttt{\_\_init\_\_.py}.
.2 experiments.
.3 \texttt{\_\_init\_\_.py}.
.2 trainers.
.3 \texttt{\_\_init\_\_.py}.
.2 data.
.3 \texttt{\_\_init\_\_.py}.
}
\vspace{0.7em}


The modular architecture of a deep learning project, characterized by the structured folder system, separates the key components into four distinct modules: models, experiments, trainers, and data. This enhances code legibility and comprehensibility, enabling other researchers to quickly access the desired code components. This approach leads to the formation of three standalone modules for models, trainers, and data, each possessing the ability to operate independently. Meanwhile, the fourth module, experiments, functions to harmonize the three modules and provide a unified and comprehensive experiment. The modular structure of independence further enables the sharing of models, trainers, and data loaders across various projects and experiments, thereby enhancing the code's reusability and scalability. 

\subsection{Case Study}
 To demonstrate the effectiveness of the convention, we present a case study of a deep learning project built using Yerbamate, and compare it to an existing project that did not adhere to the convention. Our results presented at Appendix \ref{transfer-study} illustrates that the project built with Yerbamate had better code quality, was more modular, and was easier to maintain and reproduce.
 
\subsection{Example Project Structure}

The following illustration presents a sample folder structure for a deep learning vision project\footnote{ \url{https://github.com/oalee/deep-vision}}. It's important to note that each folder represents a modular code component with a specific concern, which is demonstrated by the presence of \verb|__init__.py| within the folder.

\dirtree{%
.1 /.
.2 data.
.3 cifar10.
.4 \texttt{data\_loader.py}.
.4 \texttt{\_\_init\_\_.py}.
.3 cifar100.
.4 \texttt{data\_loader.py}.
.4 \texttt{\_\_init\_\_.py}.
.3 \texttt{\_\_init\_\_.py}.
.2 models.
.3 resnet.
.4 \texttt{fine\_tune.py}.
.4 \texttt{resnet.py}.
.4 \texttt{\_\_init\_\_.py}.
.3 efficientnet.
.4 \texttt{efficientnet.py}.
.4 \texttt{\_\_init\_\_.py}.
.3 \texttt{\_\_init\_\_.py}.
.2 trainers.
.3 classification.
.4 \texttt{pl\_classification\_module.py}.
.4 \texttt{\_\_init\_\_.py}.
.3 \texttt{\_\_init\_\_.py}.
.2 experiments.
.3 \texttt{resnet\_cifar10.py}.
.3 \texttt{resnet\_cifar100.py}.
.3 \texttt{fine\_tune\_resnet\_cifar.py}.
.3 \texttt{keras\_efficient\_net\_cifar.py}.
.3 \texttt{\_\_init\_\_.py}.
.2 \texttt{\_\_init\_\_.py}.
}
\vspace{2em}
Each top-level module can be divided into multiple sub-modules, allowing for the separation of individual components within each module. For instance, the models module can comprise sub-modules such as ResNet (\cite{resnet}), ViT (\cite{dosovitskiy2020vit}), CvT (\cite{wu2021cvt}), and others, implemented as independent components. Likewise, the data module can encompass sub-modules such as data loaders, preprocessing pipelines, and post-processing routines. This modular design promotes increased organization and manageability of the codebase, contributing to the efficiency and effectiveness of the deep learning project.


\subsection{Yerbamaté: An Open Science Python Framework}
Yerbamaté\footnote{\url{https://github.com/oalee/yerbamate}} is a result of the study on reproducibility and the importance of modularity in AI and machine learning projects. Yerbamaté is an open source open science framework designed to streamline and simplify the development and management of artificial intelligence and machine learning projects. Built around the principles of modularity and separation of concerns, Yerbamaté provides a convenient and efficient means of adding source code and dependencies to projects.

Modularity allows for a clean and organized codebase, making it easier to maintain, scale, and reuse the code in the future. In addition, Yerbamaté provides an easy-to-use interface for sharing the source code of models, trainers, data loaders, and experiments between modular projects, enabling greater flexibility and collaboration. Furthermore, adopting the Yerbamaté framework ensures that all projects adhering to its principles are readily compatible with Colab\footnote{\url{https://colab.research.google.com/github/oalee/yerbamate/blob/main/deep_learning.ipynb}}, a cloud-based platform widely used for machine learning experimentation and development.

Yerbamaté also provides support for full customizability and reproducibility of results through the inclusion of dependencies in your project. The tool supports pip and conda for dependency management, making it easy to manage and install the necessary dependencies for your project.
% Additionally, Yerbamaté is fully compatible with python, and can be used with popular libraries such as PyTorch/Lightning, TensorFlow/Keras, JAX/Flax, Huggingface/transformers. Another feature of Yerbamaté is its convenient environment management through the Yerbamaté Environment API. This API allows for the creation and management of virtual environments, ensuring that each experiment is run in an isolated environment with the necessary dependencies. 
For a comprehensive understanding of the Yerbamaté, see the documentation\footnote{\url{https://oalee.github.io/yerbamate/}}.


\subsubsection{Flexibility and Customization}
One of the primary challenges in the development of Yerbamaté was ensuring flexibility and customization.
To offer increased flexibility, Yerbamaté does not impose any restrictions on additional module names, allowing researchers to utilize their preferred module names for custom tasks. For instance, in the case of analyzing neural network weights and activations, researchers can use the "analyze" module and share code under that name.  Additionally, Yerbamaté is designed to be framework-agnostic, meaning it can work out of the box with any machine learning framework or library.  Moreover, Yerbamaté offers compatibility with Python and Python can be used for running experiments. 



\subsubsection{Yerbamaté Command Line}
The Yerbamaté command line provides useful utility functions and support for modular projects, which can facilitate the development of machine learning models.
Here is a list of the key Yerbamaté command line options:

\begin{itemize}
\item \textbf{mate init module\_name}: Initializes a new empty modular project skeleton with the given module name. 

\item \textbf{mate install url -y\textbar n\textbar o pm}: Installs a module from a git repository. Supports multiple formats for the repository URL. The flags -y, -n, and -o specify whether to skip confirmation, skip installing python dependencies, and overwrite existing code modules, respectively. The pm argument specifies the package manager to use.
\item \textbf{mate list}: Lists all available modules in the project. 
\item \textbf{mate exports}: Generates dependencies for reproduciblity and sharing.
\item \textbf{mate test exp\_module exp}: Runs the experiment specified by exp in the module exp\_module. Equivalent to python -m root\_module.exp\_module.exp test.
\item \textbf{mate train exp\_module exp}: Runs the experiment specified by exp in the module exp\_module. Equivalent to python -m root\_module.exp\_module.exp train.
\end{itemize}



Yerbamaté's install command is a crucial component of the tool, allowing for effortless installation of modularized projects that adhere to the principles of separation of concerns. With just one command, you can install a model along with its Python dependencies, making it easier to share and export modules. Additionally, Yerbamaté also supports the installation of coupled modules as whole modules. For example, you can install the source code of over 100 torch image models \footnote{\url{https://github.com/rwightman/pytorch-image-models/tree/main/timm/}} and over 30 implementation PyTorch vision in transformers \footnote{\url{https://github.com/lucidrains/vit-pytorch/tree/main/vit\_pytorch/}} directly into your project. However, the limitation of installing non-Yerbamaté projects is that Yerbamaté cannot install Python dependencies out of the box and this modules can only be installed as a whole, and sub-modules, such as models, are not installable as a standalone module since they are not independent.





\subsubsection{Yerbamaté Environment API}

The Yerbamaté Environment API is a tool designed to manage environment variables within a experiment. It prioritizes the use of an env.json file for storing environment variables, but if it is not found, it falls back to the operating system's environment variables. The API offers a convenient way to set, retrieve, and manage these variables in a centralized and organized manner. This can be particularly useful in storing and accessing environment-specific information, such as API keys, database URLs, and other sensitive data, thus ensuring that the application operates optimally regardless of the environment in which it is executed. The Yerbamaté Environment API can be easily accessed within experiments, providing a seamless and efficient method for managing environment variables within a project. 





\subsubsection{Example Custom Data Preprocessing}
The modular structure of the Yerbamaté toolkit, coupled with its compatibility with pure Python, allows for the integration of custom data preprocessing pipelines with ease. By utilizing the Yerbamaté environment API, developers and researchers can readily access the data paths and results path for the destination of their processed data. For instance, the following project structure can utilize the command \texttt{python -m deepnet.data.my\_data.preprocessing} to execute a custom preprocessing pipeline. The flexibility offered by the python modularity enables users to efficiently tailor their preprocessing procedures to the specific requirements of their research or application, and the Yerbamaté toolkit can be used to share these pipelines effortlessly.

\dirtree{%
.1 deepnet.
.2 data.
.3 \texttt{\_\_init\_\_.py}.
.3 my\_data.
.4 \texttt{\_\_init\_\_.py}.
.4 preprocessing.
.5 \texttt{\_\_init\_\_.py}.
.5 preprocess.py.
.4 data\_loader.
.2 models.
.2 trainers.
.2 experiments.
}
