
\section{Motivation}


The reproducibility crisis in AI has led to an increasing focus on open science practices to improve the validity and reproduciblity of research (\cite{coro2020open,braun2018open,hicks2021open,paton2019open,kocak2022transparency,stodden-towardreprodicibleresearch}. Moreover, the gap of software engineering knowledge among researchers and practitioners often leads to poorly structured and documented AI projects, resulting in reproducibility issues, bugs, and invalidation of research. (\cite{leakage-recrisis,epskamp2019reproducibilitybug, seAIsurvey, martinez2022softwareAI,mainatiblity}).
 


The absence of widely adopted software engineering principles in AI research has resulted in barriers to collaboration and building upon previous research (\cite{accountabilityInAi}). Open-source AI research projects often create their own command-line tools or frameworks to experiment with different models and hyperparameters, which can result in comprehensive options but also limitations. This often leads to the reinvention of the wheel, as each project implements the same tasks slightly differently for their specific needs.


This heterogeneity in software engineering practices poses a significant obstacle to code reuse and collaboration among researchers. In some cases, AI codes are of low quality, resembling spaghetti code that is hard to maintain (\cite{seAIsurvey,martinez2022softwareAI,amershi2019software,mainatiblity,leakage-recrisis,gezici2022systematicsoftware}). In contrast, others exhibit a more modular design, which can enhance code quality and maintainability (\cite{seAIsurvey,martinez2022softwareAI,wan2019does}). Moreover, the problem arises when attempting to use another researcher's model, data augmentation, or a specific approach, as it requires learning a new framework for hyperparameter configuration and execution. The lack of standardized software engineering practices increases the difficulty of reproducing and building upon previous research.
