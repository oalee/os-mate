

\onecolumn

\section{Appendix}\label{appendix}



\subsection{Yerbamaté CLI}\label{CLIApp}

Yerbamaté CLI provides utility functions and supports modular projects, facilitating machine learning model development. Key CLI options include:

\begin{itemize}
\item \texttt{mate init module\_name}: Initializes a new empty modular project skeleton with the given module name. 

\item \texttt{mate install url -y\textbar n\textbar o pm}: Installs a module from a git repository. Supports multiple formats for the repository URL. The flags -y, -n, and -o specify whether to skip confirmation, skip installing python dependencies, and overwrite existing code modules, respectively. The pm argument specifies the Python package manager to use.
\item \texttt{mate list}: Lists all available modules in the project. 
\item \texttt{mate exports}: Generates dependencies for reproduciblity and sharing.
\item \texttt{mate test exp\_module exp}: Runs the experiment specified by exp in the module exp\_module. Equivalent to \texttt{python -m root\_module.exp\_module.exp test}
\item \texttt{mate train exp\_module exp}: Runs the experiment specified by exp in the module exp\_module. Equivalent to \texttt{python -m root\_module.exp\_module.exp train}
\end{itemize}

\subsubsection{Example Commands}

\begin{itemize}
    \item \textbf{Installing GAN experiment from a modular project}: 
    
    \texttt{mate install oalee/lightweight-gan/lgan/experiments/lgan -yo pip}
    \item \textbf{Training the GAN experiment}: 
    
    \texttt{mate train lgan cars}
    
    \item \textbf{Installing transfer learning experiment}

    \texttt{mate install oalee/big\_transfer/experiments/bit}
    
    \item \textbf{Installing +100 models and code from non modular project code}

    \texttt{mate install https://github.com/rwightman/pytorch-image-models/tree/main/timm/}

    \item \textbf{Installing +30 Pytorch ViT model source code}
    
    \texttt{mate install https://github.com/lucidrains/vit-pytorch/tree/main/vit\_pytorch/}

\end{itemize}


\subsection{Examples}


\subsubsection{Exported Modules}

Yerbamate's mate export command generates a markdown table of reusable components, showcasing metadata such as the module name, type, short URL for installation, exact versions of dependencies for reproducibility, and URLs of code module dependencies. This metadata enhances transparency and reproducibility of research by providing detailed information about each component used in a project. For example, the experiment module may have a dependency on a code module, which is included in the metadata table with its associated information.

% {p{0.35\linewidth} | p{0.6\linewidth}}

\begin{figure}[H]
    \centering

\begin{tabular}{|l|l|l|p{0.35\linewidth}|}
\hline
 name           & type        & short\_url                                          & dependencies                                                                                                                                                                                                                                                                                                                             \\
\hline
 cifar10        & data        & oalee/deep-vision/deepnet/data/cifar10             & ['pytorch\_lightning\textasciitilde{}=1.7.5', 'ipdb\textasciitilde{}=0.13.9', 'torch\textasciitilde{}=1.12.1', 'torchvision\textasciitilde{}=0.13.1']                                                                                                                                                                                                                                                         \\ 
  \hline 
 
 cifar100       & data        & oalee/deep-vision/deepnet/data/cifar100            & ['pytorch\_lightning\textasciitilde{}=1.7.5', 'ipdb\textasciitilde{}=0.13.9', 'torch\textasciitilde{}=1.12.1', 'torchvision\textasciitilde{}=0.13.1']                                                                                                                                                                                                                                                         \\
  \hline 
 keras          & data        & oalee/deep-vision/deepnet/data/keras               & ['tensorflow\_gpu\textasciitilde{}=2.10.0']                                                                                                                                                                                                                                                                                                                   \\
  \hline 
 timm\_aug       & data        & oalee/deep-vision/deepnet/data/timm\_aug            & ['numpy\textasciitilde{}=1.24.2']                                                                                                                                                                                                                                                                                                                            \\
  \hline 
 resnet         & experiments & oalee/deep-vision/deepnet/experiments/resnet       & ['pytorch\_lightning\textasciitilde{}=1.7.5', 'timm\textasciitilde{}=0.6.12', 'torch\textasciitilde{}=1.12.1', 'tensorboard\textasciitilde{}=2.10.0', 'torchvision\textasciitilde{}=0.13.1', 'https://github.com/oalee/deep-vision/tree/main/deepnet/data/cifar10', 'https://github.com/oalee/deep-vision/tree/main/deepnet/trainers/classification', 'https://github.com/oalee/deep-vision/tree/main/deepnet/models/resnet'] \\
 
  \hline 
 
 resnet\_keras   & experiments & oalee/deep-vision/deepnet/experiments/resnet\_keras & ['keras\textasciitilde{}=2.10.0', 'https://github.com/oalee/deep-vision/tree/main/deepnet/models/keras/resnet']                                                                                                                                                                                                                                              \\
  \hline 
 kerasnet       & experiments & oalee/deep-vision/deepnet/experiments/kerasnet     & ['ipdb\textasciitilde{}=0.13.9', 'tensorflow\_gpu\textasciitilde{}=2.10.0', 'keras\textasciitilde{}=2.10.0']                                                                                                                                                                                                                                                                                  \\
  \hline 
 timm           & experiments & oalee/deep-vision/deepnet/experiments/timm         & ['pytorch\_lightning\textasciitilde{}=1.7.5', 'torch\textasciitilde{}=1.12.1', 'https://github.com/oalee/deep-vision/tree/main/timm/', 'https://github.com/oalee/deep-vision/tree/main/deepnet/data/cifar10', 'https://github.com/oalee/deep-vision/tree/main/deepnet/trainers/classification', 'https://github.com/oalee/deep-vision/tree/main/deepnet/models/resnet']       \\
 resnet         & models      & oalee/deep-vision/deepnet/models/resnet            & ['torch\textasciitilde{}=1.12.1']                                                                                                                                                                                                                                                                                                                            \\
  \hline 
 keras          & models      & oalee/deep-vision/deepnet/models/keras             & ['torch\textasciitilde{}=1.12.1', 'tensorflow\_gpu\textasciitilde{}=2.10.0']                                                                                                                                                                                                                                                                                                  \\
  \hline 
 vit\_pytorch    & models      & oalee/deep-vision/deepnet/models/vit\_pytorch       & ['einops\textasciitilde{}=0.4.1', 'torch\textasciitilde{}=1.12.1', 'torchvision\textasciitilde{}=0.13.1']                                                                                                                                                                                                                                                                                    \\
  \hline 
 torch\_vit      & models      & oalee/deep-vision/deepnet/models/torch\_vit         & ['einops\textasciitilde{}=0.4.1', 'torch\textasciitilde{}=1.12.1', 'torchvision\textasciitilde{}=0.13.1']                                                                                                                                                                                                                                                                                    \\
  \hline 
 classification & trainers    & oalee/deep-vision/deepnet/trainers/classification  & ['pytorch\_lightning\textasciitilde{}=1.7.5', 'torchmetrics\textasciitilde{}=0.9.3', 'torch\textasciitilde{}=1.12.1', 'ipdb\textasciitilde{}=0.13.9']                                                                                                                                                                                                                                                         \\
\hline

\end{tabular}
   
\caption{Exported module metadata generated from a modular project. The following example is generated from this repository: \url{https://github.com/oalee/deep-vision} }
    \label{fig:my_label}
\end{figure}


\subsubsection{Example Custom Data Preprocessing}
The modular structure of the Yerbamaté toolkit, coupled with its compatibility with pure Python, allows for the integration of custom data preprocessing pipelines with ease. By utilizing the Yerbamaté environment API, developers and researchers can readily access the data paths and results path for the destination of their processed data. For instance, the following project structure illustrated at Figure \ref{customdata} can utilize the command \texttt{python -m deepnet.data.my\_data.preprocessing} to execute a custom preprocessing pipeline. The flexibility offered by the python modularity enables users to efficiently tailor their preprocessing procedures to the specific requirements of their research or application, and the Yerbamaté toolkit can be used to share these pipelines effortlessly.

% \captionsetup[figure]{position=bottom,justification=centering,width=.4\textwidth,labelfont=bf,font=small}

\begin{figure}[H]
\centering
\framebox[\0.4\textwidth]{%
\begin{minipage}{0.4\textwidth}
\dirtree{%
.1 deepnet.
.2 data.
.3 \texttt{\_\_init\_\_.py}.
.3 my\_data.
.4 \texttt{\_\_init\_\_.py}.
.4 preprocessing.
.5 \texttt{\_\_init\_\_.py}.
.5 preprocess.py.
.4 data\_loader.
.2 models.
.2 trainers.
.2 experiments.
}
\end{minipage}
}
\caption{
Custom data preprocessing modular structure example
}
\label{customdata}
\end{figure}

\subsubsection{Example Keras Fine Tuning}

The following example showcases the flexibility and compatibility of Yerbamaté with the Keras framework. In this experiment definition, the models and trainers do not need to be explicitly defined, as Keras provides these functionalities out of the box. This highlights the ease of integration and adaptability of Yerbamaté with existing frameworks.

\begin{minted}{python}
import os
from tensorflow import keras
from keras.applications.resnet import ResNet50
import yerbamate

env = yerbamate.Environment()
resnet: keras.Model = ResNet50(
    include_top=False,
    input_tensor=keras.Input(shape=(32, 32, 3)),
    classes=10,
    classifier_activation="softmax",
)
resnet.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.0004, beta_1=0.9, beta_2=0.999),
    loss="binary_crossentropy",
    metrics=["accuracy", "loss"],
)
(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()
model_path = os.path.join(env.results, "model.h5")


if env.train:
    resnet.fit(
        x=x_train,
        y=y_train,
        validation_data=(x_test, y_test),
        batch_size=64,
        epochs=10,
    )
    resnet.save(model_path)

if env.test:
    resnet.load_weights(model_path)
    resnet.evaluate(x_test, y_test)

\end{minted}


% \subsubsection{Example JAX Experiment}
% The following experiment showcases the use of the experiment format with JAX framework

\subsubsection{Example GAN Experiment}

The following example illustrates the experiment definition of a Lightweight Generative Adversarial Networks (\cite{lgan,goodfellow2020generative}) implemented with Pytorch Lightning. The use of Python enables the customization of model hyperparameters, loggers, model savers, learning rate schedulers, and optimization algorithms through argument specqification in functions or classes. The source code for the complete project is accessible on Github\footnote{\url{https://github.com/oalee/lightweight-gan}} and all its modules can be installed and the experiment can be trained using Yerbamaté command line on Colab or local machines. The experiment integrates and imports independent trainers, models, and data modules, and defines the experiment's hyperparameters.

% [
% frame=lines,
% framesep=2mm,
% baselinestretch=1.2,
% % bgcolor=LightGray,
% fontsize=\footnotesize,
% linenos
% ]


\begin{minted}{python}
from ...data.cars import CarsLightningDataModule, AugWrapper
from ...trainers.lgan import LightningGanModule
from ...models.lgan import Generator, Discriminator
from torch import nn
import yerbamate, torch, pytorch_lightning as pl, pytorch_lightning.callbacks as pl_callbacks, os
# Managing environment variables
env = yerbamate.Environment()

data_module = CarsLightningDataModule(
    image_size=128,
    aug_prob=0.5,
    in_channels=3,
    data_dir=env["data_dir"],
    batch_size=8,
)

generator = Generator(
    image_size=128,
    latent_dim=128,
    fmap_max=256,
    fmap_inverse_coef=12,
    transparent=False,
    greyscale=False,
    attn_res_layers=[],
    freq_chan_attn=False,
    norm_class=nn.BatchNorm2d,
)

discriminator = Discriminator(
    image_size=128,
    fmap_max=256,
    fmap_inverse_coef=12,
    transparent=False,
    greyscale=False,
    disc_output_size=5,
    attn_res_layers=[],  # Try [16, 32, 64, 128, 256] if your hardware allows
)

g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
d_optimizer = torch.optim.Adam(
    discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999)
)

model = LightningGanModule(
    save_dir=env["results"],
    sample_interval=100,
    generator=generator,
    discriminator=AugWrapper(discriminator),
    optimizer=[
        {
            "optimizer": g_optimizer,
            "lr_scheduler": {
                "scheduler": torch.optim.lr_scheduler.StepLR(
                    g_optimizer, step_size=100, gamma=0.5
                ),
                "monitor": "fid",
            },
        },
        {
            "optimizer": d_optimizer,
            "lr_scheduler": {
                "scheduler": torch.optim.lr_scheduler.ReduceLROnPlateau(
                    d_optimizer, mode="min", factor=0.5, patience=5, verbose=True
                ),
                "monitor": "fid",
            },
        },
    ],
    aug_types=["translation", "cutout", "color", "offset"],
    aug_prob=0.5,
)

logger = pl.loggers.TensorBoardLogger(env["results"], name=env.name)
callbacks = [
    pl_callbacks.ModelCheckpoint(
        monitor="fid",
        dirpath=env["results"],
        save_top_k=1,
        mode="min",
        save_last=True,
    ),
    pl_callbacks.LearningRateMonitor(logging_interval="step"),
]
trainer = pl.Trainer(
    logger=logger,
    accelerator="gpu",
    precision=16,
    gradient_clip_val=0.5,
    callbacks=callbacks,
    max_epochs=100,
)

if env.train:
    trainer.fit(model, data_module)
if env.test:
    trainer.test(model, data_module)
if env.restart:
    trainer.fit(model, data_module, ckpt_path=os.path.join(env["results"], "last.ckpt"))

\end{minted}





\section{Refactoring Case Study}\label{transfer-study}


% In this section, we showcase the application of modularity and separation of concerns on the official implementation of "Big Transfer (BiT): General Visual Representation Learning"\cite{transferlearning}. The source code from the official repository \footnote{\url{https://github.com/google-research/big_transfer}} has been refactored \footnote{\url{https://github.com/oalee/big_transfer}} into a modular, decoupled structure.  The original folder structure of the repository is as follows:

This section presents a case study on the application of modularity and separation of concerns to the official implementation of "Big Transfer (BiT): General Visual Representation Learning" \cite{transferlearning}. The source code from the original repository\footnote{\url{https://github.com/google-research/big_transfer}} has been refactored into a modular and decoupled structure in a separate repository\footnote{\url{https://github.com/oalee/big_transfer}}. The refactoring process was applied to the PyTorch implementation of the model, and the resulting code can be easily extended and reused. Furthermore, the repository provides the implementation of the model in three popular frameworks: PyTorch, TensorFlow, and JAX, which enhances the accessibility of the work to a wider audience. However, the lack of modularity and separation of concerns in the original implementation could reduce its accessibility and extensibility. Therefore, in this case study, we showcase the process of refactoring the PyTorch code, noting that similar processes could be applied to the TensorFlow and JAX implementations to achieve the same results. The original folder structure of the repository is as following figure:

\begin{figure}[H]
\centering
\framebox[\0.4\textwidth]{%
\begin{minipage}{0.4\textwidth}
\dirtree{%
.1 /.
.2 \texttt{bit\_common.py}.
.2 \texttt{bit\_hyperrule.py}.
.2 \texttt{bit\_pytorch}.
.3 \texttt{fewshot.py}.
.3 \texttt{\_\_init\_\_.py}.
.3 \texttt{lbtoolbox.py}.
.3 \texttt{models.py}.
.3 \texttt{requirements.txt}.
.3 \texttt{train.py}.
.2 \texttt{\_\_init\_\_.py}.
}
\end{minipage}
}
\caption{
Official Repository of BiT Project Structure for Pytorch
}
\end{figure}



\vspace{0.4em}
In this examination, we delve into the modules and Python files of the official Big Transfer repository to better understand the purpose of each one. The components are as follows:

\begin{itemize}
    \item \texttt{bit\_common.py} serves as the central point for defining an argument parser and setting up a logger for experiments. Currently, the project only supports a limited set of hyperparameter selections, which include the initial learning rate, batch size, batch split, and five datasets, namely CIFAR10, CIFAR100, Oxford\_iiit\_pet, Oxford\_flowers102, and ImageNet2012. The name of this file, however, does not accurately reflect its purpose.
    \item \texttt{bit\_hyperrule.py} is responsible for defining the learning rate scheduler and a utility function for computing the resolution of the model based on the dataset. The name of this file, once again, does not accurately reflect its purpose and separates the concern of data-related functions from the learning rate scheduling.
    \item \texttt{few\_shot.py} is specifically designed to find few-shot learning samples for the model, and its name accurately reflects its purpose.
    \item \texttt{lbtoolbox.py} handles interruptions in training and provides a chronometer interface for profiling. This component is independent and does not couple with any other part of the system.
    \item \texttt{models.py} defines the models and is also an independent component that does not couple with any other part.
    \item \texttt{train.py} is utilized for training the model. This component includes the implementation for batch splitting and can only be executed with the pre-defined hyperparameter selection.
\end{itemize}


The following code illustrates the execution of the training procedure in the original repository. The training process is initiated by the \texttt{main} function located at the end of the \texttt{train.py} file. 
\vspace{0.2em}

\begin{minted}{python}
if __name__ == "__main__":
  parser = bit_common.argparser(models.KNOWN_MODELS.keys())
  parser.add_argument("--datadir", required=True,
                      help="Path to the ImageNet data folder, preprocessed for torchvision.")
  parser.add_argument("--workers", type=int, default=8,
                      help="Number of background threads used to load data.")
  parser.add_argument("--no-save", dest="save", action="store_false")
  main(parser.parse_args())
\end{minted}
\vspace{0.2em}


Furthermore, the \texttt{main} training function exhibits a high level of coupling between its arguments and objects, as evidenced in the following code snippet of the function definition:

\begin{minted}{python}


def main(args):
  logger = bit_common.setup_logger(args)

  # Lets cuDNN benchmark conv implementations and choose the fastest.
  # Only good if sizes stay the same within the main loop!
  torch.backends.cudnn.benchmark = True

  device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
  logger.info(f"Going to train on {device}")

  train_set, valid_set, train_loader, valid_loader = mktrainval(args, logger)

  logger.info(f"Loading model from {args.model}.npz")
  model = models.KNOWN_MODELS[args.model](head_size=len(valid_set.classes), zero_head=True)
  model.load_from(np.load(f"{args.model}.npz"))

  logger.info("Moving model onto all GPUs")
  model = torch.nn.DataParallel(model)

  # Optionally resume from a checkpoint.
  # Load it to CPU first as we'll move the model to GPU later.
  # This way, we save a little bit of GPU memory when loading.
  step = 0

  # Note: no weight-decay!
  optim = torch.optim.SGD(model.parameters(), lr=0.003, momentum=0.9)

  # Resume fine-tuning if we find a saved model.
  savename = pjoin(args.logdir, args.name, "bit.pth.tar")
  ....
\end{minted}
% \vspace{0.2em}

This implementation exhibits limited adaptability as the function is dependent solely on the arguments provided through the command-line interface. The following tree structure and experiment definition showcases modularity and separation of concerns applied on this task.


\begin{figure}[H]
\centering
\framebox[\0.4\textwidth]{%
\begin{minipage}{0.4\textwidth}
\dirtree{%
.1 /\texttt{big\_transfer}.
.2 data.
.3 bit.
.4 \texttt{fewshot.py}.
.4 \texttt{\_\_init\_\_.py}.
.4 \texttt{minibatch\_fewshot.py}.
.4 \texttt{requirements.txt}.
.4 \texttt{transforms.py}.
.3 \texttt{\_\_init\_\_.py}.
.2 experiments.
.3 bit.
.4 \texttt{dependencies.json}.
.4 \texttt{few\_shot.py}.
.4 \texttt{\_\_init\_\_.py}.
.4 \texttt{learn.py}.
.4 \texttt{requirements.txt}.
.3 \texttt{\_\_init\_\_.py}.
.2 \texttt{\_\_init\_\_.py}.
.2 models.
.3 \texttt{bit\_torch}.
.4 downloader.
.5 \texttt{downloader.py}.
.5 \texttt{\_\_init\_\_.py}.
.5 \texttt{requirements.txt}.
.5 \texttt{utils.py}.
.4 \texttt{\_\_init\_\_.py}.
.4 \texttt{models.py}.
.4 \texttt{requirements.txt}.
.3 \texttt{\_\_init\_\_.py}.
.2 trainers.
.3 \textt{bit\_torch}.
.4 \texttt{\_\_init\_\_.py}.
.4 \texttt{utils.py}.
.4 \texttt{logger.py}.
.4 \texttt{lr\_schduler.py}.
.4 \texttt{requirements.txt}.
.4 \texttt{trainer.py}.
.3 \texttt{\_\_init\_\_.py}.
}
\end{minipage}
}\caption{Refactored Repository of BiT Project Structure}
\label{bitnew}
\end{figure}


\vspace{0.2em}


\begin{minted}{python}
from ...trainers.bit_torch.trainer import test, train
from ...models.bit_torch.models import load_trained_model, get_model_list
from ...data.bit import get_transforms, mini_batch_fewshot
import torchvision as tv, yerbamate, os, tensorboard
from torch.utils.tensorboard import SummaryWriter


# BigTransfer Medium ResNet50 Width 1
model_name = "BiT-M-R50x1"
# Choose a model form get_model_list that can fit in to your memoery
# Try "BiT-S-R50x1" if this doesn't works for you

env = yerbamate.Environment()

train_transform, val_transform = get_transforms(img_size=[32, 32])
data_set = tv.datasets.CIFAR10(
    env["datadir"], train=True, download=True, transform=train_transform
)
val_set = tv.datasets.CIFAR10(env["datadir"], train=False, transform=val_transform)

train_set, val_set, train_loader, val_loader = mini_batch_fewshot(
    train_set=data_set,
    valid_set=val_set,
    examples_per_class=None,  # Fewshot disabled
    batch=128,
    batch_split=2,
    workers=os.cpu_count(),  # Auto-val to cpu count
)

imagenet_weight_path = os.path.join(env["weights_path"], f"{model_name}.npz")
model = load_trained_model(
    weight_path=imagenet_weight_path, model_name=model_name, num_classes=10
)
logger = SummaryWriter(log_dir=env["results"], comment=env.name)

if env.train:
    train(
        model=model,
        train_loader=train_loader,
        valid_loader=val_loader,
        train_set_size=len(train_set),
        save=True,
        save_path=os.path.join(env["results"], f"trained_{model_name}.pt"),
        batch_split=2,
        base_lr=0.001,
        eval_every=100,
        log_path=os.path.join(env["results"], "log.txt"),
        tensorboardlogger=logger,
    )

if env.test:
    test(
        model=model,
        val_loader=val_loader,
        save_path=os.path.join(env["results"], f"trained_{model_name}.pt"),
        log_path=os.path.join(env["results"], "log.txt"),
        tensorboardlogger=logger,
    )

\end{minted}




The new structure in the refactored repository depicted at Figure \ref{bitnew} is designed to address the limitations of the original implementation. By separating concerns and adopting modularization, the refactored repository provides a more flexible and scalable solution for training models. The different components in the new structure, such as the trainer, model, and data loading modules, are designed to be independent and reusable, making it easier to manage and maintain the codebase. Moreover, the experimentation module provides a unified interface for combining and executing the various components, making it easier to experiment with different configurations and hyperparameters. Overall, the new structure in the refactored repository represents a significant improvement over the original implementation, offering a better and more organized approach to training machine learning models.

