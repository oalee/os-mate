
\section{Background Information}


\subsection{Crisis of Reproducibility in AI}
The crisis of reproducibility in AI refers to the difficulty in reproducing the results of AI research (\cite{gundersen2018reproducible}). The lack of transparency in data collection and research has greatly contributed to the crisis of reproducibility in AI (\cite{gundersen2018reproducible,hutson2018artificial,leakage-recrisis}). Many AI models are developed close sourced using proprietary data and methods, making it difficult for others to replicate the research and understand the inner workings of the models (\cite{gundersen2018reproducible,accountabilityInAi}). Additionally, the lack of transparency in the research process can lead to issues such as unreliable or biased methods data, which can further undermine the credibility and reproducibility of the research, and it can decrease the trust in the field as the results of the research are not independently verifiable  (\cite{accountabilityInAi,leakage-recrisis,scully-debt-ml}). The pressure to publish results and the lack of incentives to share data and code can discourage researchers from making their work easily reproducible. (\cite{psychology-reproducibility-crisis, friesike2015open,kwon2021incentive, ali2017motivating,o2017evaluation})



\subsection{Open Science}

Open science is a research methodology that prioritizes transparency, collaboration, and reproducibility (\cite{nielsen2011reinventing}). The promotion of open science in the field of AI has garnered considerable attention in recent years (\cite{accountabilityInAi,gundersen2018reproducible,leakage-recrisis,scully-debt-ml,stodden-towardreprodicibleresearch,coro2020open,braun2018open,hicks2021open,burgelman2019open}). In the field of AI, open science practices can help to address concerns about biased or unreliable data, as well as provide a way for researchers to collaborate and build reproducible research and enhance accountability in AI (\cite{accountabilityInAi,stodden-towardreprodicibleresearch}).  Open science encourages researchers to share their knowledge, data, code, and detailed documentation of their methods (\cite{hutson2018artificial,accountabilityInAi}). 
Researchers can also use open-source frameworks and standard evaluation metrics (\cite{gundersen2018reproducible}) to facilitate reproducibility. Furthermore, the scientific community can encourage reproducibility by valuing it in the peer-review process (\cite{scully-debt-ml}), and by giving credit to researchers who share their data and code (\cite{scully-debt-ml,credit-datasharing,stodden-towardreprodicibleresearch}).




\subsection{Fair Comparison of Models in AI}

Data is a crucial factor in the success of deep learning models (\cite{lecun2015deep}). The quality, pre-processing, and augmentations applied to the data can significantly impact the model's ability to extract knowledge and make accurate predictions (\cite{shorten2019survey}). Therefore, it is essential for researchers to consistently use the same data split, pre-processing and augmentations when comparing models to ensure fair comparisons (\cite{caton2020fairness,mehrabi2021survey, leakage-recrisis}).


\subsection{The FAIR Guiding Principles for scientific data management and stewardship }
The FAIR Guiding Principles for scientific data management and stewardship were introduced in 2016 as a guideline for improving the findability, accessibility, interoperability, and reusability of digital assets, with a focus on machine-actionability to enable computational systems to find, access, interoperate, and reuse data with minimal human intervention \cite{wilkinson2016fair}. The principles have been endorsed by a diverse set of stakeholders, including academia, industry, funding agencies, and scholarly publishers. The principles emphasize the importance of metadata in making data and digital assets easy to find, with machine-readable metadata being a crucial component of the FAIRification process \cite{wilkinson2016fair}. In addition to findability, the principles also address the accessibility of data, including the need for authentication and authorization for accessing digital assets \cite{wilkinson2016fair}. Interoperability is another critical aspect of the principles, with data needing to be integrated with other data and interoperable with applications or workflows for analysis, storage, and processing \cite{wilkinson2016fair}. The ultimate goal of the FAIR principles is to optimize the reuse of data, and well-described metadata and data are essential for data replication and reuse in different settings \cite{wilkinson2016fair}. The FAIR principles refer to three types of entities, including data or any digital object, metadata, which is information about the digital object, and infrastructure \cite{wilkinson2016fair}.

\subsection{The Significance of Open Science for AI}

 
Open science represents a crucial component in the pursuit of responsible and trustworthy AI (\cite{floridi2019establishing,coro2020open,braun2018open,hicks2021open}). By prioritizing transparency and reproducibility, researchers in the field can advance its development in a safer and trustworthy manner (\cite{coro2020open,floridi2018ai4people,kocak2022transparency,stodden-towardreprodicibleresearch}).
Open science practices serve to mitigate the risks associated with closed-source AI and big data bias, which have raised significant concerns among stakeholders (\cite{batarseh2020data, o2017weapons}). Adoption of open science and open source AI can increase the fairness and impartiality of AI models (\cite{stodden-towardreprodicibleresearch,accountabilityInAi,gundersen2018reproducible}) and enhance the credibility and trustworthiness of their outputs among stakeholders and decision-makers (\cite{goodman2017european,hsiao2018vtaiwan,praprotnikevaluation}). % 

\subsection{Open Source}
Open source is a development methodology that emphasizes collaboration, transparency, and community involvement. It involves the sharing of software code and the ability to view and modify that code, as opposed to proprietary or closed-source software where the code is not available for viewing or modification. Open source has been a driving force behind many technological advancements, with the internet itself being built on open source software such as Linux..
In the field of artificial intelligence, open source development has also played a significant role. Many popular machine learning and deep learning frameworks and libraries, such as Torch, Keras, Hugging Face, and Jax, are open source, allowing for a wider range of use cases, contributions from the community, and access to cutting-edge research. The open-source nature of these frameworks also enables a more transparent and collaborative development process.


\subsection{Software Engineering}
Software engineering is a well-established discipline that encompasses the process of designing, developing, testing, and maintaining software systems with a focus on quality, reliability, and efficiency \cite{pressman2010software}. While the specific activities and methodologies involved in software engineering can vary depending on the type of software, the principles of good software engineering practices are generally applicable across all types of software (\cite{pressman2010software}), including those in the field of artificial intelligence (\cite{se4dl,wan2019does,martinez2022softwareAI,davis2011understandingmodularity}). The software engineering practices employed in the development of AI systems include, but are not limited to, testing, debugging, documentation, version control, and code review. Additionally, given the complex and evolving nature of AI systems, specific attention must be given to software requirements and their evolution over time (\cite{heyn2021requirement,belani2019requirements}). However, unlike traditional software engineering, the requirements of AI systems may not always be well-defined, and software engineering practices may need to be adapted to the rapidly changing needs of these systems (\cite{heyn2021requirement,belani2019requirements}). 


\subsubsection{Separation of Concerns}
 The separation of concerns (SoC) is a software engineering principle that suggests that different aspects of a system should be separated into distinct components, allowing for increased clarity, maintainability, and scalability of code (\cite{pressman2010software, de2002importance}). 
 In the context of AI, this principle can be applied by separating different concerns of a AI into reusable components. By adhering to SoC, researchers can improve the clarity of their code and reduce the risk of introducing bugs and errors (\cite{mo2016decoupling,mo2016decoupling,pressman2010software, de2002importance}).

\subsubsection{Modularity}
Modularity, or the practice of creating reusable components, is a fundamental aspect of software engineering (\cite{pressman2010software}). By breaking down complex systems into smaller, reusable components, researchers can improve the understandability and extendability of their code. Additionally, modular code is more easily testable and maintainable, leading to increased reproducibility and reliability of results (\cite{amershi2019software,pressman2010software}). 
\subsubsection{Modualirty in Python}
In Python, modular design can be achieved through the use of functions, modules, and libraries (\cite{sanner1999python}). 
A python module contains definitions, functions, classes, and variables (\cite{raschka2015python}). By convention, modules are stored in separate directories, and a directory containing one or more modules is called a package. The presence of \verb|__init__.py| file in a package directory indicates that it is a package, and all files in the directory are considered modules of that package. In other words, the \verb|__init__.py| file makes the directory it's in a Python package, and any code in that file is executed when the package is imported.



\subsection{Developer Experience}
The concept of developer experience (DX) is a multidimensional construct that refers to developers' perceptions of various aspects of the development process, including the usability and effectiveness of the tools, frameworks, and platforms used (\cite{fagerholm2012developer}). DX is a critical factor in software development as it has the potential to impact productivity, motivation, and satisfaction (\cite{fagerholm2012developer}).



\subsection{Decoupling Concerns in Writing AI Code}


Decoupling concerns, also known as SoC, is a crucial design principle in the field of artificial intelligence and machine learning (\cite{mo2016decoupling,qian2006decoupling, pressman2010software}. For instance, in a typical deep learning experiment, the trainer component is responsible for training the model. The trainer component can be designed to receive either a string representing the dataset/model names or the actual dataset/model objects, with the latter approach providing greater flexibility and customization. Another example of separation of concerns in AI is the data loading and augmentation process, which can be hardcoded into the data loading module or passed as an object to a function. Similarly, a deep learning model can be implemented as a monolithic block of code or as a series of modular components, such as the encoder, decoder, and attention mechanism. The latter approach allows for greater flexibility and customization of the model, as each component can be modified or replaced without affecting the other components.



\section{Related Work}

In recent years, there has been an increasing demand for project structure templates to provide guidance and organization to data science projects. This trend has been towards modularization and separation of concerns to make projects more manageable and easier to maintain. This has been exemplified by frameworks like Cookiecutter, which separates the codebase into four main concerns, and Towards Data Science, which provides a comprehensive structure for data science projects. 

\subsection{Coockiecutter}
% Cookiecutter is a popular tool that helps in setting up project directory structures and boilerplate code for Python-based projects. It provides a predefined directory structure, which includes nootbooks, reports, figures, references, license, and others, to help developers with setting up a consistent project directory structure. In contrast to Yerbamate, Cookiecutter separates the src directory into four concerns and modules: data, models, notebooks, and src, while Yerbamate creates an arbitrary number of independent modules. The predefined structure of Cookiecutter helps with separating out the different aspects of a project, such as data, models, and code, into their own directories. This can help make a project more organized and easier to navigate. However, the modularity provided by Yerbamate allows for more flexibility in how a project is structured, as it allows for the creation of independent modules without being restricted to a predefined structure.
Cookiecutter is a popular tool for setting up project directory structures and boilerplate code for Python-based projects. It provides a predefined directory structure that includes notebooks, reports, figures, references, license, and others to help developers set up a consistent project directory structure. The Cookiecutter project directory structure separates the src directory into four predefined modules: data, features, models, and visualization, providing a clear separation of concerns. In contrast, Yerbamate allows for the creation of an arbitrary number of independent modules, making it more flexible and easier to manage and reuse code in different projects. Yerbamate can be integrated into the Cookiecutter project structure, managing the src module of the project, and merging the design of license, docs, notebooks, and references. 



\subsection{Software Engineering Practices For Accountable AI}

Recent research has highlighted the significance of software engineering practices for developing transparent and accountable AI systems. "Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure" proposes a comprehensive guide for documenting the process of dataset development to ensure accountability and mitigate dataset risks. The dataset development lifecycle includes five stages: requirements analysis, design, implementation, testing, and maintenance. The authors recommend creating a comprehensive set of documentation artifacts at each stage, with clearly designated owners and responsibilities to ensure accountability. The documentation aims to create a clear and transparent understanding of the dataset, its development, and its intended uses. The authors also provide a flexible template for documenting dataset requirements that covers a broad range of factors. The proposed documentation guideline can be integrated into the Yerbamate framework, which emphasizes transparency, reproducibility, reusability, and collaboration in the research community.

% Recent research has highlighted the significance of software engineering practices for developing transparent and accountable AI systems. The paper "Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure" proposes a comprehensive guide for documenting the process of dataset development to ensure accountability and mitigate dataset risks. The dataset development lifecycle proposed in this work includes five stages: requirements analysis, design, implementation, testing, and maintenance. To ensure accountability, the authors recommend creating a comprehensive set of documentation artifacts at each stage, with clearly designated owners and responsibilities. The primary purpose of the documentation is to create a clear and transparent understanding of the dataset, its development, and its intended uses. The authors also provide a flexible template for documenting dataset requirements that covers a broad range of qualitative and quantitative factors. The resulting documentation guidline can get integrated with the yerbamate framework.

% Recent works have emphasized the importance of software engineering practices for developing accountable and transparent AI systems. In this regard, the paper "Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure" presented a comprehensive guide for documenting the process of dataset development to mitigate dataset risks and ensure accountability.
%  presents a dataset development lifecycle consisting of five stages: requirements analysis, design, implementation, testing, and maintenance. To ensure accountability, the authors propose a comprehensive set of documentation artifacts to be created at each stage, with clearly designated owners and responsibilities. The primary purpose of the documentation is to create a clear and transparent understanding of the dataset, its development, and its intended uses. The authors also suggest a flexible template for documenting dataset requirements that covers a broad range of qualitative and quantitative factors. This framework offers a new approach to addressing the increasing demand for accountability in machine learning, allowing for greater transparency and responsible use of data.
%  The results of the Software Engineering Practices for Accountable AI paper can be integrated with the Yerbamate framework, which emphasizes reproducibility, reusability, transparency, and collaboration in the research community, and enhances developer experience.
% As AI systems become increasingly integrated into critical systems, ensuring their accountability and transparency becomes a pressing issue. The paper "Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure" proposes a set of best practices to mitigate dataset risks and improve the accountability and transparency of the development process. The paper suggests adopting a deliberate and intentional methodology throughout the dataset development lifecycle, emphasizing documentation practices, diverse oversight processes, and robust maintenance mechanisms. The proposed practices facilitate reviews and audits, providing bounds on accountability both in dataset creation and in the ML systems that depend on those datasets as infrastructure. The authors emphasize the importance of establishing a non-linear cycle of dataset development, with analysis, design, and evaluation central to the process. The proposed practices replace abductive and posteriori reasoning about dataset provenance with careful documentation and understanding of the limitations of the dataset. The paper's emphasis on accountability and transparency aligns with current efforts to improve AI's social and ethical implications, promoting a more responsible and effective use of AI technology.

% "Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure" is a research paper that explores the importance of accountability in machine learning datasets, as well as the practices and strategies that can be adopted from software engineering and infrastructure to achieve it. The paper highlights the critical role that datasets play in machine learning, serving as infrastructure and knowledge construction, and emphasizes the need for intentional and deliberate methodology in their development to avoid biases and other issues. The authors propose a set of documentation practices throughout the dataset development lifecycle and identify diverse oversight processes and maintenance mechanisms for ensuring the dataset's ongoing quality and relevance. By drawing on the lessons and experiences from software engineering, the authors aim to provide a framework for promoting accountability in machine learning datasets and improving the overall fairness and trustworthiness of AI systems.

% O
% There have been several efforts to establish best practices for software engineering in AI, including the "Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure" publication which emphasizes the importance of open science principles in the development of AI datasets and the documentation process for researcher collaboration (\cite{accountabilityInAi}). These efforts aim to establish a more standardized and transparent approach to AI development, ultimately leading to more trustworthy and reliable AI systems.
