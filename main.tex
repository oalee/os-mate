\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{algorithm} 
\usepackage{algpseudocode}
\usepackage{caption}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{siunitx}
\geometry{ margin=3cm}
\usepackage{subcaption}
\usepackage{todonotes}
\usepackage{dirtree}
\usepackage{minted}
% \usepackage{biblo}
% \graphicspath{{./images}}
\usepackage[
backend=biber,
style=ieee,
]{biblatex}


\setminted[python]{breaklines, framesep=2mm, fontsize=\footnotesize, numbersep=5pt}


\addbibresource{ref.bib}

\title{
  Master's Internship Report 

  Reproducibility Crisis in Artificial Intelligence

  
  }
\vspace{20mm}
\author{
    Ali Rahimi
    {\and}  
     \vspace{20mm}
    \includegraphics[width=50mm,scale=0.5]{umlogo.png}
    {\and}
  \vspace{25mm}Department of Advanced Computing Sciences
    {\and} 
      \vspace{5mm}  Maastricht University
    {\and} 
    Netherlands
}

\vspace{50mm}

\date{February 2023}

\begin{document}
\maketitle
\clearpage
\tableofcontents
\clearpage

\begin{abstract}
    This research internship aimed to address the reproducibility crisis in artificial intelligence (AI) research by investigating software engineering best practices, open science and accessible AI. To achieve this goal, Yerbamaté framework was developed around the best software engineering principles of modularity and separation of concerns. Yerbamaté is an open science modular Python framework designed to streamline and simplify the development and management of machine learning projects. The framework encourages quality coding, collaboration, and the sharing of models, trainers, data loaders, and knowledge, while also promoting reproducibility, customization, and flexibility. The modular design and separation of concerns simplify the development and maintenance of machine learning models, leading to an improved developer experience. The straightforward installation, sharing, and training process makes it accessible to researchers and practitioners with varying technical expertise, enhancing collaboration and knowledge sharing. The findings suggest that the adoption of modular design principles and open science tools can contribute significantly to addressing the reproducibility crisis in AI, leading to more accessible, transparent, and trustworthy AI.
\end{abstract}

\section{Introduction}

  In recent years, deep learning has emerged as a powerful tool for solving complex problems in various fields, such as image and speech recognition, natural language processing, and computer vision (\cite{lecun2015deep}). 
Deep learning projects, whilst still considered software engineering, differ from traditional software engineering in that they revolve around the creation of models that can extract knowledge from data and make predictions or decisions, as opposed to relying on a pre-defined set of instructions (\cite{lecun2015deep,amershi2019software,wan2019does,se4dl}). These disparities make some phases such as the testing phase of deep learning projects distinct from traditional software (\cite{wan2019does}). Still, principles and design patterns commonly used in software engineering can still aid in developing and maintaining deep learning software (\cite{amershi2019software,wan2019does,se4dl}). 
The widespread adoption of the Python programming language in AI programming has been noted in recent studies (\cite{mihajlovic2020use,raschka2020machine}). As such, this project focuses on the examination of software engineering practices in the context of Python and its machine learning frameworks. Many interdisciplinary researchers may lack the software engineering expertise necessary to manage their code bases to ensure correctness, understandability, extendability, and reusability (\cite{amershi2019software,scully-debt-ml,leakage-recrisis,accountabilityInAi}).
% To address these challenges, we investigated the application of software engineering principles of separation of concerns (SoC) and modular design in Python to AI projects. The goal of this study was to improve the reproducibility and reusability of python AI components through the adoption of best software engineering practices. Software engineering is the process of designing, developing, and maintaining software systems efficiently and reliably (\cite{pressman2010software}).
% In this internship, I explored software engineering practices that could help researchers share and collaborate AI code.

\section{Background Information}


\subsection{Crisis of Reproducibility in Artificial Intelligence (AI)}
The crisis of reproducibility in AI refers to the difficulty in reproducing the results of AI research (\cite{gundersen2018reproducible}). The lack of transparency in data collection and research has greatly contributed to the crisis of reproducibility in AI (\cite{gundersen2018reproducible,hutson2018artificial,leakage-recrisis}). Many AI models are developed close sourced using proprietary data and methods, making it difficult for others to replicate the research and understand the inner workings of the models (\cite{gundersen2018reproducible,accountabilityInAi}). Additionally, the lack of transparency in the data collection process can lead to issues such as biased or unreliable data, which can further undermine the credibility and reproducibility of the research, and it can decrease the trust in the field as the results of the research are not independently verifiable  (\cite{accountabilityInAi,leakage-recrisis,scully-debt-ml}). The pressure to publish results and the lack of incentives to share data and code can discourage researchers from making their work easily reproducible. (\cite{psychology-reproducibility-crisis, friesike2015open,kwon2021incentive, ali2017motivating,o2017evaluation})


\subsection{Open Science}

Open science is a research methodology that prioritizes transparency, collaboration, and reproducibility (\cite{nielsen2011reinventing}). The promotion of open science in the field of AI has garnered considerable attention in recent years (\cite{accountabilityInAi, gundersen2018reproducible, leakage-recrisis, scully-debt-ml, stodden-towardreprodicibleresearch, coro2020open, braun2018open, hicks2021open, burgelman2019open}). In the field of AI, open science practices can help to address concerns about biased or unreliable data, as well as provide a way for researchers to collaborate and build reproducible research and enhance accountability in AI (\cite{accountabilityInAi, stodden-towardreprodicibleresearch}).  Open science encourages researchers to share their knowledge, data, code, and detailed documentation of their methods (\cite{hutson2018artificial,accountabilityInAi}). 
Researchers can also use open-source frameworks and standard evaluation metrics (\cite{gundersen2018reproducible}) to facilitate reproducibility. Furthermore, the scientific community can encourage reproducibility by valuing it in the peer-review process (\cite{scully-debt-ml}), and by giving credit to researchers who share their data and code (\cite{scully-debt-ml,credit-datasharing,stodden-towardreprodicibleresearch}).




\subsection{The Significance of Data for AI}

Data is a crucial factor in the success of deep learning models (\cite{lecun2015deep}). The quality, pre-processing, and augmentations applied to the data can significantly impact the model's ability to extract knowledge and make accurate predictions (\cite{shorten2019survey}). Therefore, it is essential for researchers to consistently use the same data split, pre-processing and augmentations when comparing models to ensure fair comparisons (\cite{caton2020fairness,mehrabi2021survey, leakage-recrisis}). Furthermore, if the data collection process is not transparent and well-documented, it can lead to issues such as biased or unreliable data, which can harm the credibility and reproducibility of the research (\cite{accountabilityInAi}). 
 This is highlighted in the seminal publication "Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure," which emphasizes the importance of incorporating open science principles into the development life cycle of AI datasets and the documentation process for researcher collaboration (\cite{accountabilityInAi}).



% Additionally, bugs in the data-related code or leakage between the test and train sets can also invalidate the results of a study (\cite{leakage-recrisis}).




% \subsection{Open Science}

\subsection{The Significance of Open Science for AI}

 
Open science represents a crucial component in the pursuit of responsible and trustworthy AI (\cite{floridi2019establishing,coro2020open,braun2018open,hicks2021open}). By prioritizing transparency and reproducibility, researchers in the field can advance its development in a safer and trustworthy manner (\cite{coro2020open,floridi2018ai4people,kocak2022transparency,stodden-towardreprodicibleresearch}).
Open science practices serve to mitigate the risks associated with closed-source AI and big data bias, which have raised significant concerns among stakeholders (\cite{batarseh2020data, o2017weapons}). Adoption of open science and open source AI can increase the fairness and impartiality of AI models (\cite{stodden-towardreprodicibleresearch,accountabilityInAi,gundersen2018reproducible}) and enhance the credibility and trustworthiness of their outputs among stakeholders and decision-makers (\cite{goodman2017european,hsiao2018vtaiwan,praprotnikevaluation}). % 
% Beyond that, close-sourced AI and bias in big data can have detrimental effects and increases inequality, and threaten democracy (\cite{o2017weapons}).  
\subsection{Software Engineering}
% Software engineering is the process of designing, developing, and maintaining software systems efficiently and reliably (\cite{pressman2010software}).
Software engineering is a well-established discipline that encompasses the process of designing, developing, testing, and maintaining software systems with a focus on quality, reliability, and efficiency \cite{pressman2010software}. While the specific activities and methodologies involved in software engineering can vary depending on the type of software, the principles of good software engineering practices are generally applicable across all types of software, including those in the field of artificial intelligence. The software engineering practices employed in the development of AI systems include, but are not limited to, testing, debugging, documentation, version control, and code review. Additionally, given the complex and evolving nature of AI systems, specific attention must be given to software requirements and their evolution over time. However, unlike traditional software engineering, the requirements of AI systems may not always be well-defined, and software engineering practices may need to be adapted to the rapidly changing needs of these systems.

\subsubsection{Separation of Concerns}
 The separation of concerns (SoC) is a software engineering principle that suggests that different aspects of a system should be separated into distinct components, allowing for increased clarity, maintainability, and scalability of code (\cite{pressman2010software, de2002importance}). In the context of AI, this principle can be applied by separating the different stages of a machine learning pipeline into distinct, reusable components (\cite{mo2016decoupling,mo2016decoupling,pressman2010software, de2002importance}). By adhering to SoC, researchers can improve the clarity of their code and reduce the risk of introducing bugs and errors (\cite{mo2016decoupling,mo2016decoupling,pressman2010software, de2002importance}).

\subsubsection{Modularity in Python}
Modularity, or the practice of creating reusable components, is a fundamental aspect of software engineering (\cite{pressman2010software}). In Python, modular design can be achieved through the use of functions, modules, and libraries (\cite{sanner1999python}). By breaking down complex systems into smaller, reusable components, researchers can improve the understandability and extendability of their code. Additionally, modular code is more easily testable and maintainable, leading to increased reproducibility and reliability of results (\cite{amershi2019software,pressman2010software}).
A python module contains definitions, functions, classes, and variables (\cite{raschka2015python}). By convention, modules are stored in separate directories, and a directory containing one or more modules is called a package. The presence of \verb|__init__.py| file in a package directory indicates that it is a package, and all files in the directory are considered modules of that package. In other words, the \verb|__init__.py| file makes the directory it's in a Python package, and any code in that file is executed when the package is imported.



\subsection{Deep Learning Frameworks in Python}

The utilization of deep learning models in Python has seen the rise of several prominent frameworks, including Jax, Keras/TensorFlow, PyTorch, PyTorch Lightning, and Jax/Flax (\cite{mihajlovic2020use,raschka2020machine, nguyen2019machine, shatnawi2018comparative}). These frameworks provide a comprehensive set of tools and functionalities for the implementation and training of deep learning models (\cite{elshawi2021dlbench}). 
However, choosing the right framework can be challenging as each framework has its own advantages and limitations (\cite{nguyen2019machine, elshawi2021dlbench,shatnawi2018comparative}).

One approach to addressing this challenge is the development of unified deep learning frameworks such as Ivy (\cite{ivy}). Ivy supports multiple frameworks, enabling researchers to utilize the strengths of each framework without sacrificing compatibility and ease of use (\cite{ivy}). However, using such a unified framework also introduces overhead in learning, and computation, as well as limitations in terms of customizability. For instance, users are required to utilize Ivy tensors instead of PyTorch or Keras tensors, limiting access to low-level functionality (\cite{IvyDocs, ivy}).


\section{Methodology}

The widespread adoption of the Python programming language in AI programming has been noted in recent studies (\cite{mihajlovic2020use,raschka2020machine}). As such, this project focuses on the examination of software engineering practices in the context of Python and its machine learning frameworks. Many interdisciplinary researchers may lack the software engineering expertise necessary to manage their code bases to ensure correctness, understandability, extendability, and reusability (\cite{amershi2019software,scully-debt-ml,leakage-recrisis,accountabilityInAi}). To address these challenges, we investigated the application of software engineering principles of separation of concerns (SoC) and modular design in Python to AI projects. The goal of this study was to improve the reproducibility and reusability of python AI components through the adoption of best software engineering practices.

% \subsection{Software Engineering for Python AI Projects}
%  In the context of AI, the implementation of modularity and SoC principles results in the separation of models, data-related components, and trainers. 
% This approach facilitates the creation of a modular design, where the code is organized into distinct, reusable components (\cite{sanner1999python,pressman2010software}). This decoupling has the potential to greatly enhance the reusability of code greatly, enabling easier experimentation with models and datasets.


\subsection{A Different Approach to Deep Learning Framework}

An alternative approach to the compatibility and re-usability issue in deep learning frameworks is to enforce principles of separation of concerns (SoC) and modularity in Python. 
The implementation of modularity and SoC principles results in the separation of models, data-related components, and trainers. 
This approach facilitates the creation of a modular design, where the code is organized into distinct, reusable components (\cite{sanner1999python,pressman2010software}). This separation enhances the reusability of the code, making it easier to experiment with different models and datasets within the same framework. This approach has the advantage of limiting the learning overhead to the principles of software engineering, rather than requiring a transition to a unified deep learning framework.



A limitation of this approach is that it requires manual translation of AI components, such as data loaders and models, into different frameworks when necessary. While this approach may require additional effort, it allows for greater flexibility and customizability in implementing deep learning models. Additionally, it promotes a deeper understanding of the underlying software engineering principles and their application in the field of AI.



\section{Research Questions}

The objective of this internship at the start was to address the following research questions.

\subsection{Can a standard format be designed for data loaders and preprocessing pipelines?}
{
The development of a universal standard format for data loaders and preprocessing pipelines in deep learning is challenging due to the framework-specific nature of the task. Preprocessing pipeline design is highly dependent on the specific problem and its data, making it difficult to create a general solution. However, Separation of concerns (SoC) and modularity principles can enhance code reusability by creating the data loading as an independent python module, and separating preprocessing pipeline from data loading. This project focused on making data loading and code components, such as data loaders and the preprocessing pipelines, sharable within a python machine learning framework.
}

\subsection{Can this be put in a high-quality, easily accessible database for local access?} {
The creation of a high-quality, easily accessible database storing preprocessed datasets and related components, such as data loaders and preprocessing pipelines, is complex and involves challenges in terms of data processing, storage, and retrieval, which were beyond the scope of the project. Limitations such as credibility, scalability, compatibility with different deep learning frameworks, privacy, and storage pose additional challenges. This project focused on designing a standard for sharing code-related components, instead of making data available, for feasibility of the task.
}

\subsection{Can the database be made easily expandable?} {
Expanding the database of deep learning code components, such as data loaders, relies on adopting software engineering principles of separation of concerns (SoC) and modularity. Adhering to these principles makes code components shareable across projects within the same framework, which can get added to a database and scale in time.
}

\section{Results}



The implementation of software engineering principles of as separation of concerns (SoC) and modularity in deep learning projects results in a well-structured illustrated below. 
\dirtree{%
.1 /.
.2 models.
.3 \texttt{\_\_init\_\_.py}.
.2 experiments.
.3 \texttt{\_\_init\_\_.py}.
.2 trainers.
.3 \texttt{\_\_init\_\_.py}.
.2 data.
.3 \texttt{\_\_init\_\_.py}.
}
\vspace{0.7em}


The modular architecture of a deep learning project, characterized by the structured folder system, separates the key components into four distinct modules: models, experiments, trainers, and data. This enhances code legibility and comprehensibility, enabling other researchers to quickly access the desired code components. This approach leads to the formation of three standalone modules for models, trainers, and data, each possessing the ability to operate independently. Meanwhile, the fourth module, experiments, functions to harmonize the three modules and provide a unified and comprehensive experiment. The modular structure of independence further enables the sharing of models, trainers, and data loaders across various projects and experiments, thereby enhancing the code's reusability and scalability. 


\subsubsection{Example Project Structure}

The following illustration presents a sample folder structure for a deep learning vision project\footnote{ \url{https://github.com/oalee/deep-vision}}. It's important to note that each folder represents a modular code component with a specific concern, which is demonstrated by the presence of \verb|__init__.py| within the folder.

\dirtree{%
.1 /.
.2 data.
.3 cifar10.
.4 \texttt{data\_loader.py}.
.4 \texttt{\_\_init\_\_.py}.
.3 cifar100.
.4 \texttt{data\_loader.py}.
.4 \texttt{\_\_init\_\_.py}.
.3 \texttt{\_\_init\_\_.py}.
.2 models.
.3 resnet.
.4 \texttt{fine\_tune.py}.
.4 \texttt{resnet.py}.
.4 \texttt{\_\_init\_\_.py}.
.3 efficientnet.
.4 \texttt{efficientnet.py}.
.4 \texttt{\_\_init\_\_.py}.
.3 \texttt{\_\_init\_\_.py}.
.2 trainers.
.3 classification.
.4 \texttt{pl\_classification\_module.py}.
.4 \texttt{\_\_init\_\_.py}.
.3 \texttt{\_\_init\_\_.py}.
.2 experiments.
.3 \texttt{resnet\_cifar10.py}.
.3 \texttt{resnet\_cifar100.py}.
.3 \texttt{fine\_tune\_resnet\_cifar.py}.
.3 \texttt{keras\_efficient\_net\_cifar.py}.
.3 \texttt{\_\_init\_\_.py}.
.2 \texttt{\_\_init\_\_.py}.
}
\vspace{2em}
Each top-level module can be divided into multiple sub-modules, allowing for the separation of individual components within each module. For instance, the models module can comprise sub-modules such as ResNet (\cite{resnet}), ViT (\cite{dosovitskiy2020vit}), CvT (\cite{wu2021cvt}), and others, implemented as independent components. Likewise, the data module can encompass sub-modules such as data loaders, preprocessing pipelines, and post-processing routines. This modular design promotes increased organization and manageability of the codebase, contributing to the efficiency and effectiveness of the deep learning project.


\subsection{Yerbamaté: An Open Science Python Framework}
Yerbamaté\footnote{\url{https://github.com/oalee/yerbamate}} is a result of the study on reproducibility and the importance of modularity in AI and machine learning projects. Yerbamaté is an open source open science framework designed to streamline and simplify the development and management of artificial intelligence and machine learning projects. Built around the principles of modularity and separation of concerns, Yerbamaté provides a convenient and efficient means of adding source code and dependencies to projects. Modularity allows for a clean and organized codebase, making it easier to maintain, scale, and reuse the code in the future. In addition, Yerbamaté provides an easy-to-use interface for sharing the source code of models, trainers, data loaders, and experiments between modular projects, enabling greater flexibility and collaboration. Furthermore, adopting the Yerbamaté framework ensures that all projects adhering to its principles are readily compatible with Colab\footnote{\url{https://colab.research.google.com/github/oalee/yerbamate/blob/main/deep_learning.ipynb}}, a cloud-based platform widely used for machine learning experimentation and development.

Yerbamaté also provides support for full customizability and reproducibility of results through the inclusion of dependencies in your project. The tool supports pip and conda for dependency management, making it easy to manage and install the necessary dependencies for your project.
% Additionally, Yerbamaté is fully compatible with python, and can be used with popular libraries such as PyTorch/Lightning, TensorFlow/Keras, JAX/Flax, Huggingface/transformers. Another feature of Yerbamaté is its convenient environment management through the Yerbamaté Environment API. This API allows for the creation and management of virtual environments, ensuring that each experiment is run in an isolated environment with the necessary dependencies. 
For a comprehensive understanding of the Yerbamaté, see the documentation\footnote{\url{https://oalee.github.io/yerbamate/}}.


\subsubsection{Flexibility and Customization}
One of the primary challenges in the development of Yerbamaté was ensuring flexibility and customization.
To offer increased flexibility, Yerbamaté does not impose any restrictions on additional module names, allowing researchers to utilize their preferred module names for custom tasks. For instance, in the case of analyzing neural network weights and activations, researchers can use the "analyze" module and share code under that name.  Additionally, Yerbamaté is designed to be framework-agnostic, meaning it can work out of the box with any machine learning framework or library.  Moreover, Yerbamaté offers compatibility with Python and Python can be used for running experiments. 

\subsubsection{Yerbamaté Command Line}
The Yerbamaté command line provides useful utility functions and support for modular projects, which can facilitate the development of machine learning models.
Here is a list of the key Yerbamaté command line options:

\begin{itemize}
\item \textbf{mate init module\_name}: Initializes a new empty modular project skeleton with the given module name. 

\item \textbf{mate install url -y\textbar n\textbar o pm}: Installs a module from a git repository. Supports multiple formats for the repository URL. The flags -y, -n, and -o specify whether to skip confirmation, skip installing python dependencies, and overwrite existing code modules, respectively. The pm argument specifies the package manager to use.
\item \textbf{mate list}: Lists all available modules in the project. 
\item \textbf{mate exports}: Generates dependencies for reproduciblity and sharing.
\item \textbf{mate test exp\_module exp}: Runs the experiment specified by exp in the module exp\_module. Equivalent to python -m root\_module.exp\_module.exp test.
\item \textbf{mate train exp\_module exp}: Runs the experiment specified by exp in the module exp\_module. Equivalent to python -m root\_module.exp\_module.exp train.
\end{itemize}



Yerbamaté's install command is a crucial component of the tool, allowing for effortless installation of modularized projects that adhere to the principles of separation of concerns. With just one command, you can install a model along with its Python dependencies, making it easier to share and export modules. Additionally, Yerbamaté also supports the installation of coupled modules as whole modules. For example, you can install the source code of over 100 torch image models \footnote{\url{https://github.com/rwightman/pytorch-image-models/tree/main/timm/}} and over 30 implementation PyTorch vision in transformers \footnote{\url{https://github.com/lucidrains/vit-pytorch/tree/main/vit\_pytorch/}} directly into your project. However, the limitation of installing non-Yerbamaté projects is that Yerbamaté cannot install Python dependencies out of the box and this modules can only be installed as a whole, and sub-modules, such as models, are not installable as a standalone module since they are not independent.

\begin{itemize}
    \item \textbf{Installing GAN experiment from a modular project}: 

    \texttt{mate install oalee/lightweight-gan/lgan/experiments/lgan -yo pip}

    \item \textbf{Training the GAN experiment}: 
    
    \texttt{mate train lgan cars}
    
    \item \textbf{Installing transfer learning experiment}

    \texttt{mate install oalee/big\_transfer/experiments/bit}
    
    \item \textbf{Installing code from non modular project code}

    \texttt{mate install https://github.com/rwightman/pytorch-image-models/tree/main/timm/}

    \item \textbf{Installing pytorch ViT implementation source code}
    
    \texttt{mate install https://github.com/lucidrains/vit-pytorch/tree/main/vit\_pytorch/}

\end{itemize}





\subsubsection{Yerbamaté Environment API}

The Yerbamaté Environment API is a tool designed to manage environment variables within a experiment. It prioritizes the use of an env.json file for storing environment variables, but if it is not found, it falls back to the operating system's environment variables. The API offers a convenient way to set, retrieve, and manage these variables in a centralized and organized manner. This can be particularly useful in storing and accessing environment-specific information, such as API keys, database URLs, and other sensitive data, thus ensuring that the application operates optimally regardless of the environment in which it is executed. The Yerbamaté Environment API can be easily accessed within experiments, providing a seamless and efficient method for managing environment variables within a project. 





\subsection{Experiment Configuration}

In the field of artificial intelligence and machine learning, defining hyperparameters and experiments plays a crucial role in the development and optimization of models \cite{wu2019hyperparameter}. The choice of format for defining experiments and hyperparameters can greatly impact the efficiency and flexibility of the experimentation process. After evaluating various formats such as JSON and TOML, it has been determined that Python is the most suitable format for defining experiments in AI and ML. Python provides a high level of expressiveness and versatility, allowing for easy modifications and adaptations of the experiment definition. In addition, Python offers a straightforward syntax for defining hyperparameters and allows for the integration of existing code and libraries. This greatly reduces the overhead associated with switching between different languages or systems, leading to a more streamlined and efficient experimentation process. Furthermore, the ability to use Python's powerful libraries and modules enhances the experimentation process by providing access to a vast range of tools and resources. This further enables the exploration of a wider range of hyperparameters and models, leading to a more comprehensive understanding of the problem at hand.

\subsubsection{No-Loop Python Experiment Configuration Convention}

The experiment configuration in this study adopts a restricted version of the Python language that only disallows the use of loops as a convention. The experiment configuration design choice is intended to maintain a hyperparameter-focused experiment format while promoting separation of concerns. The resulting approach provides several benefits, including increased flexibility and customization, as the configuration format can be adapted to various AI tasks, custom use cases, frameworks, and libraries. Additionally, by incorporating well-documented Python code, the readability of the configuration file is improved, making it more accessible to researchers and practitioners alike. Furthermore, this format is executable directly with Python, which makes it Turing complete. This property is particularly useful because it means that the format can include arbitrary computation and is capable of expressing any algorithm, enhancing its flexibility and power.

\subsubsection{Example Experiment}

The following example illustrates the experiment definition of a Lightweight Generative Adversarial Networks (\cite{lgan,goodfellow2020generative}) implemented with Pytorch Lightning. The use of Python enables the customization of model hyperparameters, loggers, model savers, learning rate schedulers, and optimization algorithms through argument specqification in functions or classes. The source code for the complete project is accessible on Github\footnote{\url{https://github.com/oalee/lightweight-gan}} and all its modules can be installed and the experiment can be trained using Yerbamaté command line on Colab or local machines. The experiment integrates and imports independent trainers, models, and data modules, and defines the experiment's hyperparameters.

\begin{minted}{python}
from ...data.cars import CarsLightningDataModule, AugWrapper
from ...trainers.lgan import LightningGanModule
from ...models.lgan import Generator, Discriminator
from torch import nn
import yerbamate, torch, pytorch_lightning as pl, pytorch_lightning.callbacks as pl_callbacks, os
# Managing environment variables
env = yerbamate.Environment()

data_module = CarsLightningDataModule(
    image_size=128,
    aug_prob=0.5,
    in_channels=3,
    data_dir=env["data_dir"],
    batch_size=8,
)

generator = Generator(
    image_size=128,
    latent_dim=128,
    fmap_max=256,
    fmap_inverse_coef=12,
    transparent=False,
    greyscale=False,
    attn_res_layers=[],
    freq_chan_attn=False,
    norm_class=nn.BatchNorm2d,
)

discriminator = Discriminator(
    image_size=128,
    fmap_max=256,
    fmap_inverse_coef=12,
    transparent=False,
    greyscale=False,
    disc_output_size=5,
    attn_res_layers=[],  # Try [16, 32, 64, 128, 256] if your hardware allows
)

g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
d_optimizer = torch.optim.Adam(
    discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999)
)

model = LightningGanModule(
    save_dir=env["results"],
    sample_interval=100,
    generator=generator,
    discriminator=AugWrapper(discriminator),
    optimizer=[
        {
            "optimizer": g_optimizer,
            "lr_scheduler": {
                "scheduler": torch.optim.lr_scheduler.StepLR(
                    g_optimizer, step_size=100, gamma=0.5
                ),
                "monitor": "fid",
            },
        },
        {
            "optimizer": d_optimizer,
            "lr_scheduler": {
                "scheduler": torch.optim.lr_scheduler.ReduceLROnPlateau(
                    d_optimizer, mode="min", factor=0.5, patience=5, verbose=True
                ),
                "monitor": "fid",
            },
        },
    ],
    aug_types=["translation", "cutout", "color", "offset"],
    aug_prob=0.5,
)

logger = pl.loggers.TensorBoardLogger(env["results"], name=env.name)
callbacks = [
    pl_callbacks.ModelCheckpoint(
        monitor="fid",
        dirpath=env["results"],
        save_top_k=1,
        mode="min",
        save_last=True,
    ),
    pl_callbacks.LearningRateMonitor(logging_interval="step"),
]
trainer = pl.Trainer(
    logger=logger,
    accelerator="gpu",
    precision=16,
    gradient_clip_val=0.5,
    callbacks=callbacks,
    max_epochs=100,
)

if env.train:
    trainer.fit(model, data_module)
if env.test:
    trainer.test(model, data_module)
if env.restart:
    trainer.fit(model, data_module, ckpt_path=os.path.join(env["results"], "last.ckpt"))

\end{minted}



\subsection{Seperation of Concerns for AI}

In the field of software engineering, including artificial intelligence and machine learning, decoupling concerns, also known as separation of concerns, is a crucial design principle that helps to improve the maintainability, scalability, and reusability of code (\cite{mo2016decoupling,qian2006decoupling, pressman2010software}. The goal of decoupling concerns is to break down complex systems into smaller, modular components that can be independently developed, tested, and maintained (\cite{pressman2010software, mo2016decoupling, qian2006decoupling}). For example, in a typical deep learning experiment, the trainer component is responsible for training the model. The trainer component can be designed to receive either a string representing the dataset name or the actual dataset objects. The latter approach provides greater flexibility and customization, as the trainer component can operate on any dataset objects, regardless of the underlying dataset.


Another example of separation of concerns in AI is the data loading, training and augmentation process. In this context, hardcoding the dataset and augmentation within the training code leads to limitations in terms of flexibility and experimentation. A more effective approach is to pass the data objects directly to the trainer, rather than relying on a string representation of the dataset name. This approach offers greater flexibility in the data loading process, enabling more customization and control, as well as ease of experimentation with different datasets.

The separation of concerns also applies to the design of the model components. For example, a deep learning model can be implemented as a monolithic block of code or as a series of modular components, such as the encoder, decoder, and attention mechanism. The latter approach allows for greater flexibility and customization of the model, as each component can be modified or replaced without affecting the other components. Decoupling concerns is a crucial design principle in the field of artificial intelligence and machine learning. 

\subsubsection{Example Transfer Learning}

In this section, we showcase the application of modularity and separation of concerns on the official implementation of "Big Transfer (BiT): General Visual Representation Learning"\cite{transferlearning}. The source code from the official repository \footnote{\url{https://github.com/google-research/big_transfer}} has been refactored \footnote{\url{https://github.com/oalee/big_transfer}} into a modular, decoupled structure.  The original folder structure of the repository is as follows:

\dirtree{%
.1 /.
.2 \texttt{bit\_common.py}.
.2 \texttt{bit\_hyperrule.py}.
.2 \texttt{bit\_pytorch}.
.3 \texttt{fewshot.py}.
.3 \texttt{\_\_init\_\_.py}.
.3 \texttt{lbtoolbox.py}.
.3 \texttt{models.py}.
.3 \texttt{requirements.txt}.
.3 \texttt{train.py}.
.2 \texttt{\_\_init\_\_.py}.
}
\vspace{0.4em}
In this examination, we delve into the components of the official Big Transfer repository to better understand the purpose of each one. The components are as follows:

\begin{itemize}
    \item \texttt{bit\_common.py} serves as the central point for defining an argument parser and setting up a logger for experiments. Currently, the project only supports a limited set of hyperparameter selections, which include the initial learning rate, batch size, batch split, and five datasets, namely CIFAR10, CIFAR100, Oxford\_iiit\_pet, Oxford\_flowers102, and ImageNet2012. The name of this file, however, does not accurately reflect its purpose.
    \item \texttt{bit\_hyperrule.py} is responsible for defining the learning rate scheduler and a utility function for computing the resolution of the model based on the dataset. The name of this file, once again, does not accurately reflect its purpose and separates the concern of data-related functions from the learning rate scheduling.
    \item \texttt{few\_shot.py} is specifically designed to find few-shot learning samples for the model, and its name accurately reflects its purpose.
    \item \texttt{lbtoolbox.py} handles interruptions in training and provides a chronometer interface for profiling. This component is independent and does not couple with any other part of the system.
    \item \texttt{models.py} defines the models and is also an independent component that does not couple with any other part.
    \item \texttt{train.py} is utilized for training the model. This component includes the implementation for batch splitting and can only be executed with the pre-defined hyperparameter selection.
\end{itemize}


The following code illustrates the execution of the training procedure in the original repository. The training process is initiated by the \texttt{main} function located at the end of the \texttt{train.py} file. 
\begin{minted}{python}
if __name__ == "__main__":
  parser = bit_common.argparser(models.KNOWN_MODELS.keys())
  parser.add_argument("--datadir", required=True,
                      help="Path to the ImageNet data folder, preprocessed for torchvision.")
  parser.add_argument("--workers", type=int, default=8,
                      help="Number of background threads used to load data.")
  parser.add_argument("--no-save", dest="save", action="store_false")
  main(parser.parse_args())
\end{minted}

This implementation exhibits limited adaptability as the function is dependent solely on the arguments provided through the command-line interface. The following tree structure and experiment definition showcases modularity and separation of concerns applied on this task .

\vspace{0.5em}
\dirtree{%
.1 /\texttt{big\_transfer}.
.2 data.
.3 bit.
.4 \texttt{fewshot.py}.
.4 \texttt{\_\_init\_\_.py}.
.4 \texttt{minibatch\_fewshot.py}.
.4 \texttt{requirements.txt}.
.4 \texttt{transforms.py}.
.3 \texttt{\_\_init\_\_.py}.
.2 experiments.
.3 bit.
.4 \texttt{dependencies.json}.
.4 \texttt{few\_shot.py}.
.4 \texttt{\_\_init\_\_.py}.
.4 \texttt{learn.py}.
.4 \texttt{requirements.txt}.
.3 \texttt{\_\_init\_\_.py}.
.2 \texttt{\_\_init\_\_.py}.
.2 models.
.3 \textt{bit\_torch}.
.4 downloader.
.5 \texttt{downloader.py}.
.5 \texttt{\_\_init\_\_.py}.
.5 \texttt{requirements.txt}.
.5 \texttt{utils.py}.
.4 \texttt{\_\_init\_\_.py}.
.4 \texttt{models.py}.
.4 \texttt{requirements.txt}.
.3 \texttt{\_\_init\_\_.py}.
.2 trainers.
.3 \textt{bit\_torch}.
.4 \texttt{\_\_init\_\_.py}.
.4 \texttt{utils.py}.
.4 \texttt{logger.py}.
.4 \texttt{lr\_schduler.py}.
.4 \texttt{requirements.txt}.
.4 \texttt{trainer.py}.
.3 \texttt{\_\_init\_\_.py}.
}

\begin{minted}{python}
from ...trainers.bit_torch.trainer import test, train
from ...models.bit_torch.models import load_trained_model, get_model_list
from ...data.bit import get_transforms, mini_batch_fewshot
import torchvision as tv, yerbamate, os, tensorboard
from torch.utils.tensorboard import SummaryWriter


# BigTransfer Medium ResNet50 Width 1
model_name = "BiT-M-R50x1"
# Choose a model form get_model_list that can fit in to your memoery
# Try "BiT-S-R50x1" if this doesn't works for you

env = yerbamate.Environment()

train_transform, val_transform = get_transforms(img_size=[32, 32])
data_set = tv.datasets.CIFAR10(
    env["datadir"], train=True, download=True, transform=train_transform
)
val_set = tv.datasets.CIFAR10(env["datadir"], train=False, transform=val_transform)

train_set, val_set, train_loader, val_loader = mini_batch_fewshot(
    train_set=data_set,
    valid_set=val_set,
    examples_per_class=None,  # Fewshot disabled
    batch=128,
    batch_split=2,
    workers=os.cpu_count(),  # Auto-val to cpu count
)

imagenet_weight_path = os.path.join(env["weights_path"], f"{model_name}.npz")
model = load_trained_model(
    weight_path=imagenet_weight_path, model_name=model_name, num_classes=10
)
logger = SummaryWriter(log_dir=env["results"], comment=env.name)

if env.train:
    train(
        model=model,
        train_loader=train_loader,
        valid_loader=val_loader,
        train_set_size=len(train_set),
        save=True,
        save_path=os.path.join(env["results"], f"trained_{model_name}.pt"),
        batch_split=2,
        base_lr=0.001,
        eval_every=100,
        log_path=os.path.join(env["results"], "log.txt"),
        tensorboardlogger=logger,
    )

if env.test:
    test(
        model=model,
        val_loader=val_loader,
        save_path=os.path.join(env["results"], f"trained_{model_name}.pt"),
        log_path=os.path.join(env["results"], "log.txt"),
        tensorboardlogger=logger,
    )

\end{minted}


The new structure in the refactored repository is designed to address the limitations of the original implementation. By separating concerns and adopting modularization, the refactored repository provides a more flexible and scalable solution for training models. The different components in the new structure, such as the trainer, model, and data loading modules, are designed to be independent and reusable, making it easier to manage and maintain the codebase. Moreover, the experimentation module provides a unified interface for combining and executing the various components, making it easier to experiment with different configurations and hyperparameters. Overall, the new structure in the refactored repository represents a significant improvement over the original implementation, offering a better and more organized approach to training machine learning models.

\subsubsection{Example Custom Data Preprocessing}
The modular structure of the Yerbamaté toolkit, coupled with its compatibility with pure Python, allows for the integration of custom data preprocessing pipelines with ease. By utilizing the Yerbamaté environment API, developers and researchers can readily access the data paths and results path for the destination of their processed data. For instance, the following project structure can utilize the command \texttt{python -m deepnet.data.my\_data.preprocessing} to execute a custom preprocessing pipeline. The flexibility offered by the python modularity enables users to efficiently tailor their preprocessing procedures to the specific requirements of their research or application, and the Yerbamaté toolkit can be used to share these pipelines effortlessly.

\dirtree{%
.1 deepnet.
.2 data.
.3 \texttt{\_\_init\_\_.py}.
.3 my\_data.
.4 \texttt{\_\_init\_\_.py}.
.4 preprocessing.
.5 \texttt{\_\_init\_\_.py}.
.5 preprocess.py.
.4 data\_loader.
.2 models.
.2 trainers.
.2 experiments.
}

\section{Social Impact}
Machine learning models and artificial intelligence systems have the potential to impact society, both positively and negatively significantly (\cite{mittelstadt2019principles, jobin2019global,arrieta2020explainable, floridi2018ai4people}). Consequently, AI's ethical and societal implications have been the subject of much discussion and research in recent years (\cite{floridi2018ai4people, goodman2017european, floridi2019establishing, mittelstadt2016ethics}).
Open science can significantly impact society by promoting collaboration, transparency, and accessibility in research, enabling broader participation in the scientific process and sharing of knowledge and tools, thus accelerating safer progress and (\cite{kocak2022transparency, wachter2017transparent, coro2020open, braun2018open, paton2019open, goodman2017european}). In artificial intelligence, open science can help democratize access to machine learning and facilitate the development of ethical and accountable AI systems (\cite{goodman2017european, batarseh2020data}). By promoting the principles of open science, researchers and practitioners can work together to build more robust, trustworthy, and fair AI solutions (\cite{accountabilityInAi, kocak2022transparency,wachter2017transparent, coro2020open, braun2018open, hicks2021open, goodman2017european}.
The development of open source, open science, accessible and user-friendly tools for training machine learning models has the potential to democratize access to artificial intelligence and facilitate more involvement in research and innovation. The Yerbamaté toolkit contributes in this regard, providing a simple and effective means for researchers and practitioners to share, develop and evaluate machine learning models. Moreover, it makes training AI accessible to a broader audience as anyone can run an experiment on accessible science tools such as Colab\footnote{\url{https://colab.research.google.com/github/oalee/yerbamate/blob/main/deep_learning.ipynb}} and reproduce a scientific experiment and conduct their own experiments.
The wide accessibility of AI through Yerbamaté and other similar open science tools have the potential to accelerate research in various fields (\cite{olson2018system, wolf2020designing,morris2020ai,ong2021guide, li2018can}) including healthcare (\cite{haristiani2020combining}), law (\cite{ashley2017legal}), and education (\cite{goel2020ai}). For example, machine learning models can improve patient outcomes (\cite{hamet2017medicine}), detect fraud in financial transactions (\cite{bao2022fraudartificial}, and enhance personalized learning in education (\cite{haristiani2020combining, goel2020ai}). At the same time, it is essential to ensure that the development and application of AI adhere to ethical principles, including inclusion, transparency, accountability, and fairness (\cite{ accountabilityInAi,  jobin2019global, floridi2018ai4people, wachter2017transparent, arrieta2020explainable, mittelstadt2019principles, goodman2017european, mittelstadt2016ethics, floridi2019establishing, o2017weapons}).


\section{Conclusion}
In conclusion, the development of the Yerbamaté framework could offer a significant contribution to the field of artificial intelligence by promoting open science and accessible AI. By providing a modular framework that encourages best software engineering practices, Yerbamaté simplifies the development and maintenance of machine learning models, enhancing collaboration and knowledge sharing among researchers and practitioners with varying technical expertise. The framework's ease of use and flexibility enables wider accessibility to artificial intelligence, making it possible for individuals and organizations to develop and train machine learning models with greater ease and efficiency. Overall, the adoption of modular design principles and open science tools has the potential to revolutionize the development and implementation of trustworthy and transparent AI, facilitating innovation and progress in various fields. Nevertheless, the widespread adoption of the Yerbamaté toolkit and other similar tools may pose challenges, including a potential learning curve, implementation overhead, and a need for further refinement and evaluation. One area for future work could be the creation of naming conventions for modules and functions, which could further enhance the modularity and readability of the code. Additionally, comprehensive evaluation and testing of the framework could help to identify any potential issues or areas for improvement, ensuring the continued effectiveness and usefulness of the Yerbamaté toolkit.
 
\newpage
% \vspace{5em}

\printbibliography




\end{document}
